{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/","title":"LLM Training Scripts","text":"<p>[!note] Auto-generated Documentation This documentation was automatically generated from the codebase on 2025-03-13.</p> <p>The LLM Training Scripts are used to generate training data from our various repositories for fine-tuning large language models on our codebase.</p>","tags":["documentation","llm","training-scripts","auto-generated"]},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/#key-components","title":"Key Components","text":"<ul> <li>LLM-Training-Scripts/README|Overview</li> <li>LLM-Training-Scripts/data_generation|Data Generation Module</li> <li>LLM-Training-Scripts/root|Core Utilities</li> </ul>","tags":["documentation","llm","training-scripts","auto-generated"]},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/#dependencies","title":"Dependencies","text":"<ul> <li>Python 3.9+</li> <li>OpenAI API</li> <li>Claude API (optional)</li> <li>tiktoken, tqdm, rich</li> </ul>","tags":["documentation","llm","training-scripts","auto-generated"]},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/#related-resources","title":"Related Resources","text":"<ul> <li>EHS Dashboard (Public)/API|API Documentation</li> <li>EHS Dashboard (Public)/Resources/Request Templates|Request Templates</li> </ul>","tags":["documentation","llm","training-scripts","auto-generated"]},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/","title":"Training Scripts Documentation","text":"<p>This documentation was automatically generated from the codebase.</p>","tags":["documentation","auto-generated"]},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/#modules","title":"Modules","text":"<ul> <li>root</li> <li>data_generation</li> </ul>","tags":["documentation","auto-generated"]},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/","title":"data_generation Documentation","text":"<p>This module contains the following files:</p> <ul> <li>data_generation/extractor</li> <li>data_generation/parser</li> <li>data_generation/progress_ui</li> <li>data_generation/templates</li> <li>data_generation/tokenizer</li> <li>data_generation/training_generator</li> </ul>","tags":["documentation","module","auto-generated"]},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/extractor/","title":"Extractor","text":"<pre><code>---\ntags: \n  - python\n  - data-extraction\n  - documentation\n---\n</code></pre>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/extractor/#data_generationextractorpy","title":"data_generation/extractor.py","text":"<p>This Python script is responsible for extracting and reading files from various repositories, focusing on specific file types and excluding certain directories. The extracted content is then returned as a list of tuples, where each tuple contains the file path and the file content.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/extractor/#constants","title":"Constants","text":"<p>The script defines several constants that are used throughout the code:</p> <ul> <li><code>MIN_CONTENT_LENGTH</code>: The minimum length of the file content to be considered for extraction.</li> <li><code>MAX_FILE_SIZE_MB</code>: The maximum file size (in megabytes) for a file to be included in the extraction process.</li> <li><code>REPOS</code>: A dictionary that defines the repository paths and the file extensions to be extracted from each repository.</li> <li><code>EXCLUDED_DIRS</code>: A set of directory names that should be excluded from the extraction process.</li> </ul>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/extractor/#functions","title":"Functions","text":""},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/extractor/#should_skip_filefilepath","title":"<code>should_skip_file(filepath)</code>","text":"<p>This function determines whether a file should be skipped based on its size or the directory it is located in. It checks if the file is located in any of the <code>EXCLUDED_DIRS</code> and if the file size exceeds the <code>MAX_FILE_SIZE_MB</code> limit.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/extractor/#extract_docsrepo_path-extensions","title":"<code>extract_docs(repo_path, extensions)</code>","text":"<p>This function is responsible for extracting and reading the files from a given repository path, based on the provided file extensions. It uses the <code>glob.glob()</code> function to find all the files matching the specified pattern, and then reads the content of each file. If the file content length is greater than or equal to the <code>MIN_CONTENT_LENGTH</code>, the file path and content are added to the <code>docs</code> list, which is returned at the end of the function.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/extractor/#usage","title":"Usage","text":"<p>To use this script, you would typically call the <code>extract_docs()</code> function for each repository defined in the <code>REPOS</code> dictionary, and then process the extracted content as needed. For example:</p> <pre><code>for repo_name, repo_info in REPOS.items():\n    docs = extract_docs(repo_info[\"path\"], repo_info[\"extensions\"])\n    # Process the extracted documents as needed\n    for filepath, content in docs:\n        print(f\"File: {filepath}\")\n        print(f\"Content: {content}\")\n</code></pre> <p>This would extract the files from all the repositories defined in the <code>REPOS</code> dictionary and print the file path and content for each extracted document.</p> <p>[!note] This script is designed to be used as part of a larger data extraction and processing pipeline. It is important to ensure that the repository paths and file extensions are configured correctly for your specific use case.</p> <p>data_generation/README</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/parser/","title":"Parser","text":"<pre><code>---\ntags: \n  - python\n  - data-processing\n  - text-cleaning\n---\n</code></pre>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/parser/#data_generationparserpy","title":"<code>data_generation/parser.py</code>","text":"<p>This Python file contains utility functions for cleaning and extracting leading comments from text content. It is likely used as part of a larger data processing or generation pipeline, where the ability to normalize text and extract metadata from source files is important.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/parser/#clean_texttext","title":"<code>clean_text(text)</code>","text":"<p>This function takes a string <code>text</code> as input and returns a cleaned version of the text. It normalizes whitespace by replacing all consecutive whitespace characters with a single space, and then strips any leading or trailing whitespace.</p> <pre><code>def clean_text(text):\n    \"\"\"Clean text by normalizing whitespace and stripping excess newlines.\"\"\"\n    return re.sub(r'\\s+', ' ', text.strip())\n</code></pre>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/parser/#extract_leading_commentcontent-file_ext","title":"<code>extract_leading_comment(content, file_ext)</code>","text":"<p>This function takes two arguments: <code>content</code>, which is the raw text content of a file, and <code>file_ext</code>, which is the file extension of the content. It then attempts to extract the leading comment from the file, if applicable, based on the file extension.</p> <p>The function uses a dictionary of regular expression patterns to match the leading comment for various file types, including <code>.rb</code> (Ruby), <code>.js</code> (JavaScript), <code>.jsx</code> (React JSX), and <code>.tsx</code> (React TSX). If a matching pattern is found, the function returns the leading comment, stripped of any surrounding whitespace, followed by an extra newline. If no leading comment is found, the function returns an empty string.</p> <pre><code>def extract_leading_comment(content, file_ext):\n    \"\"\"Extract the leading comment from a file if applicable.\"\"\"\n    patterns = {\n        \".rb\": r'^(#.*\\n)+',\n        \".js\": r'^((//.*\\n)+|(/\\*.*?\\*/\\n))',\n        \".jsx\": r'^((//.*\\n)+|(/\\*.*?\\*/\\n))',\n        \".tsx\": r'^((//.*\\n)+|(/\\*.*?\\*/\\n))'\n    }\n    pattern = patterns.get(file_ext)\n    if pattern:\n        match = re.match(pattern, content, re.DOTALL)\n        if match:\n            return match.group(0).strip() + \"\\n\\n\"\n    return \"\"\n</code></pre> <p>[!note] This file is part of a larger data processing or generation pipeline, and the functions provided here are likely used to prepare text content for further processing or analysis.</p> <p>For more information on the overall data processing pipeline, see the documentation for the data_generation/main.py file.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/progress_ui/","title":"Progress ui","text":"<pre><code>---\ntags: \n  - python\n  - progress\n  - ui\n  - data-generation\n---\n\n# `data_generation/progress_ui.py`\n\nThis Python file defines a set of functions that create an elegant and interactive progress tracking UI using the Rich library. The UI includes a rotating ASCII animation, per-repository progress bars, and a dashboard that displays live statistics and sample data. This file is part of the data generation process for a larger project.\n\n## `rotating_ascii()`\n\nThis function continuously rotates a set of ASCII frames to create a fun and engaging animation. It runs in a separate thread to avoid blocking the main execution.\n\n&gt; [!note]\n&gt; The `ASCII_FRAMES` and `ASCII_CYCLE` variables are used to define the animation sequence and cycle through the frames.\n\n## `setup_progress()`\n\nThis function sets up a Rich `Progress` object with a custom layout, including a progress bar, task description, and time elapsed column.\n\n## `initialize_repo_tasks(progress, repo_stats)`\n\nThis function creates individual progress bars for each repository based on the provided `repo_stats` dictionary. It adds the tasks to the `progress` object and returns a dictionary of the created tasks.\n\n## `get_dashboard(progress, stats, repo_stats, repo_tasks, data_samples)`\n\nThis function builds a refined Rich UI dashboard layout that includes the overall progress, live statistics, and a sample of the generated data. The dashboard is composed of several `Panel` objects arranged in a `Group`.\n\n&gt; [!note]\n&gt; The `file_type_samples` dictionary is used to display one sample per file type, showing the filename and a concise preview of the assistant's response.\n\n## `start_ascii_animation()`\n\nThis function starts the ASCII animation in a separate thread, allowing it to run continuously without blocking the main execution.\n\n## Dependencies and Design Patterns\n\nThis file relies on the following external libraries:\n\n- [rich](rich.md): A Python library for creating rich terminal UIs, including progress bars, panels, and text formatting.\n- `itertools.cycle`: A built-in Python function that creates an infinite iterator that cycles through a sequence.\n- `threading`: A built-in Python module for creating and managing threads.\n\nThe code follows a modular design, with each function responsible for a specific task. This allows for better maintainability and testability. The use of threads ensures that the ASCII animation runs continuously without blocking the main execution.\n\n## Related Files\n\n- [data_generation/data_generator.py](data_generation/data_generator.py.md): The main data generation script that uses the progress tracking UI defined in this file.\n\n```python\n# progress_ui.py\nimport os\nimport json\nfrom rich.console import Console, Group\nfrom rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn\nfrom rich.panel import Panel\nfrom itertools import cycle\nimport time\nimport threading\n\nconsole = Console()\n\n# Fun ASCII Animation for Engagement\nASCII_FRAMES = [\"\ud83d\ude80  \", \"\ud83d\ude80\ud83d\ude80 \", \"\ud83d\ude80\ud83d\ude80\ud83d\ude80\", \"  \ud83d\ude80\ud83d\ude80\", \"   \ud83d\ude80\", \"    \ud83d\ude80\"]\nASCII_CYCLE = cycle(ASCII_FRAMES)\n\ndef rotating_ascii():\n    \"\"\"Continuously rotates the ASCII animation for a fun effect.\"\"\"\n    while True:\n        console.print(f\"[bold cyan]{next(ASCII_CYCLE)}[/bold cyan]\", end=\"\\r\")\n        time.sleep(0.2)\n\ndef setup_progress():\n    \"\"\"Set up an elegant Rich progress tracking UI.\"\"\"\n    progress = Progress(\n        BarColumn(),\n        TextColumn(\"[bold blue]{task.description}\"),\n        TimeElapsedColumn(),\n    )\n    return progress\n\ndef initialize_repo_tasks(progress, repo_stats):\n    \"\"\"Creates individual progress bars for each repository.\"\"\"\n    repo_tasks = {}\n    for repo_name, data in repo_stats.items():\n        repo_tasks[repo_name] = progress.add_task(\n            f\"[green]{repo_name}[/green]\", total=data[\"total_files\"]\n        )\n    return repo_tasks\n\ndef get_dashboard(progress, stats, repo_stats, repo_tasks, data_samples):\n    \"\"\"\n    Builds a refined Rich UI dashboard layout with per-repo progress tracking and a sample data preview.\n    The header has been removed from the group to prevent duplication.\n    \"\"\"\n    stats_text = f\"\"\"\n\ud83d\udcc4 Files Processed:  [bold yellow]{stats['files_processed']} / {stats['total_files']}[/bold yellow]\n\ud83d\udcc2 Data Processed:   [bold cyan]{stats['processed_size']:.2f}MB / {stats['total_size']:.2f}MB[/bold cyan]\n\ud83d\udcdd Training Pairs:   [bold cyan]{stats['training_pairs']}[/bold cyan]\n\u2705 Completion:       [bold blue]{stats['completion']}%[/bold blue]\n\ud83d\udd04 Current Repo:     [bold red]{stats['current_repo']}[/bold red]\n\ud83d\udcc4 Current File:     [bold magenta]{stats['current_file']}[/bold magenta]\n    \"\"\"\n    stats_panel = Panel(stats_text, title=\"\ud83d\udcca Live Statistics\", border_style=\"green\")\n\n    # Show ONE sample per file type (keyed by extension)\n    file_type_samples = {}\n    for sample in data_samples:\n        file_ext = os.path.splitext(sample[\"filename\"])[1]\n        if file_ext not in file_type_samples:\n            file_type_samples[file_ext] = sample\n\n    sample_text = \"\\n\\n\".join(\n        f\"[bold magenta]{ext}[/bold magenta]: {sample['filename']} - \" +\n        (\n            (sample['messages'][1]['content'][:50].strip() + \"...\")\n            if len(sample['messages']) &gt; 1 and len(sample['messages'][1]['content']) &gt; 50\n            else sample['messages'][1]['content'].strip()\n        )\n        for ext, sample in file_type_samples.items()\n    ) if file_type_samples else \"No sample data available yet.\"\n\n    sample_panel = Panel(sample_text, title=\"\ud83d\udcd1 Sample Assistant Responses (Concise)\", border_style=\"blue\")\n\n    return Group(\n        Panel(progress, title=\"Overall Progress (MB)\", border_style=\"blue\"),\n        stats_panel,\n        sample_panel\n    )\n\ndef start_ascii_animation():\n    \"\"\"Starts the ASCII animation in a separate thread.\"\"\"\n    animation_thread = threading.Thread(target=rotating_ascii, daemon=True)\n    animation_thread.start()\n</code></pre>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/templates/","title":"Templates","text":""},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/templates/#yaml","title":"```yaml","text":"<p>tags:   - python   - documentation   - templates</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/templates/#data_generationtemplatespy","title":"data_generation/templates.py","text":"<p>This Python file defines a set of templates for generating various types of documentation for a given code file. These templates can be used to automatically generate summaries, technical documentation, code explanations, system interactions, security analyses, debugging tips, performance optimizations, test cases, comparative analyses, code cleanup suggestions, error handling descriptions, dependency analyses, UI/UX impact assessments, and best practices.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/templates/#templates","title":"Templates","text":"<p>The <code>TEMPLATES</code> list contains a set of dictionaries, each representing a different type of documentation template. Each template has two keys:</p> <ol> <li><code>prompt_template</code>: A string that defines the prompt for the documentation generation. This string can include placeholders for the filename, context, leading comment, and file content.</li> <li><code>completion_template</code>: A string that defines the suffix to be added to the filename to create the output file name for the generated documentation.</li> </ol>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/templates/#prompt-template-placeholders","title":"Prompt Template Placeholders","text":"<ul> <li><code>{filename}</code>: The name of the file being documented.</li> <li><code>{context}</code>: Additional context about the file, such as its location in the project.</li> <li><code>{leading_comment}</code>: The leading comment block at the top of the file.</li> <li><code>{content}</code>: The content of the file, excluding the leading comment.</li> </ul>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/templates/#template-examples","title":"Template Examples","text":"<ol> <li>Summarize the purpose and functionality of the file:    ```    Summarize the purpose and functionality of the file:    file: {filename} (context: {context})</li> </ol> <p>{leading_comment}{content}    <code>``    Output file:</code>_summary.md`</p> <ol> <li>Generate detailed technical documentation for the file:    ```    Generate detailed technical documentation for the file:    file: {filename} (context: {context})</li> </ol> <p>{leading_comment}{content}    <code>``    Output file:</code>_technical_documentation.md`</p> <ol> <li>Explain how the code in the file works, including its key functions and dependencies:    ```    Explain how the code in the file works, including its key functions and dependencies:    file: {filename} (context: {context})</li> </ol> <p>{leading_comment}{content}    <code>``    Output file:</code>_code_explanation.md`</p> <p>[!note] The remaining templates follow a similar structure, with different prompts and output file names.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/templates/#usage","title":"Usage","text":"<p>This templates.py file is likely used in a larger project or system that generates documentation automatically based on the code files. The templates can be used to generate various types of documentation for a given file, which can then be integrated into the project's overall documentation system.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/templates/#related-files","title":"Related Files","text":"<ul> <li>data_generation/generator.py</li> <li>data_generation/utils.py</li> </ul>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/templates/#best-practices","title":"Best Practices","text":"<ul> <li>Keep the templates concise and focused on specific documentation types.</li> <li>Ensure the placeholders are used consistently across all templates.</li> <li>Consider adding more templates or customizing the existing ones to fit the project's specific needs.</li> <li>Integrate the documentation generation process into the project's build or deployment workflow.</li> <li>Regularly review and update the templates to ensure they remain relevant and useful.</li> </ul>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/tokenizer/","title":"Tokenizer","text":"<pre><code>---\ntags: \n  - documentation\n  - python\n  - tokenizer\n  - data_generation\n---\n</code></pre>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/tokenizer/#data_generationtokenizerpy","title":"data_generation/tokenizer.py","text":"<p>This Python file contains a tokenizer utility for estimating the token count of text and truncating text to fit within a specified token limit. It uses the <code>tiktoken</code> library, which provides an OpenAI-compatible tokenizer, to perform these operations.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/tokenizer/#estimate_token_counttext","title":"<code>estimate_token_count(text)</code>","text":"<p>This function takes a string <code>text</code> as input and returns the estimated token count using the OpenAI tokenizer.</p> <pre><code>def estimate_token_count(text):\n    \"\"\"Estimate token count using OpenAI's tokenizer.\"\"\"\n    return len(encoding.encode(text))\n</code></pre>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/tokenizer/#truncate_text_by_tokenstext-max_tokens","title":"<code>truncate_text_by_tokens(text, max_tokens)</code>","text":"<p>This function takes a string <code>text</code> and an integer <code>max_tokens</code> as input. It truncates the text to fit within the specified token limit by encoding the text, taking the first <code>max_tokens</code> tokens, and then decoding the resulting token sequence back to a string.</p> <pre><code>def truncate_text_by_tokens(text, max_tokens):\n    \"\"\"Truncate text to fit within a token limit.\"\"\"\n    tokens = encoding.encode(text)\n    return encoding.decode(tokens[:max_tokens]) if len(tokens) &gt; max_tokens else text\n</code></pre> <p>[!note] The <code>tiktoken</code> library is used to provide an OpenAI-compatible tokenizer, which is necessary for accurately estimating token counts and truncating text for use with language models like GPT-3.5-turbo.</p> <p>This file is part of the <code>data_generation</code> module, which is responsible for generating and preprocessing data for use in machine learning models. The tokenizer functionality provided here is likely used in other parts of the data generation pipeline, such as when preparing text data for input to a language model.</p> <p>For more information on the <code>data_generation</code> module, see data_generation/README.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/training_generator/","title":"Training generator","text":""},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/training_generator/#yaml","title":"```yaml","text":"<p>tags:   - python   - data-generation   - training   - openai   - documentation</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/training_generator/#data_generationtraining_generatorpy","title":"data_generation/training_generator.py","text":"<p>This Python file is responsible for generating training data for an AI-powered code assistant. It utilizes the OpenAI API to provide dynamic responses to various prompts, such as code summarization, debugging insights, security assessments, and test case recommendations. The file also includes utilities for estimating the cost of running the training data generation process.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/training_generator/#key-components","title":"Key Components","text":""},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/training_generator/#estimate_task_cost","title":"<code>estimate_task_cost()</code>","text":"<p>This function estimates the total token usage and cost before running the training data generation process. It iterates through the available repositories, extracts the relevant files, and calculates the approximate input and output tokens, as well as the estimated cost.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/training_generator/#call_openai_apiprompt","title":"<code>call_openai_api(prompt)</code>","text":"<p>This function sends a request to the OpenAI API to generate a response based on the provided prompt. It handles any exceptions that may occur during the API call and returns the response or an error message.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/training_generator/#generate_assistant_responseprompt_type-filename-content-context","title":"<code>generate_assistant_response(prompt_type, filename, content, context)</code>","text":"<p>This function generates an AI-driven response dynamically using the OpenAI API. It extracts code statistics, selects the appropriate prompt template, and constructs the system and user prompts. The function then calls the <code>call_openai_api()</code> function to obtain the response.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/training_generator/#create_training_pairsfilepath-content-repo_name","title":"<code>create_training_pairs(filepath, content, repo_name)</code>","text":"<p>This function creates training pairs from the given file content. For demonstration purposes, it uses the 'summarize' prompt type to generate a response, which is then included in the training pair.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/training_generator/#dependencies-and-design-patterns","title":"Dependencies and Design Patterns","text":"<p>The file utilizes the following dependencies: - <code>openai</code>: The OpenAI API client for generating AI-driven responses. - <code>logging</code>: The Python logging module for handling logging and error reporting.</p> <p>The file follows a modular design, with separate functions for specific tasks, such as cost estimation, API calls, and response generation. This design promotes maintainability and testability of the codebase.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/data_generation/training_generator/#related-files","title":"Related Files","text":"<ul> <li>data_generation/parser.py</li> <li>data_generation/tokenizer.py</li> <li>data_generation/templates.py</li> <li>data_generation/extractor.py</li> </ul> <p>[!note] Ensure that the <code>OPENAI_API_KEY</code> environment variable is set before running the code, as it is required for the OpenAI API calls.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/root/","title":"Root Documentation","text":"<p>This module contains the following files:</p> <ul> <li>root/claude_prompt</li> <li>root/generate_training_data</li> <li>root/sample</li> </ul>","tags":["documentation","module","auto-generated"]},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/root/claude_prompt/","title":"Claude prompt","text":"<p>Here is the comprehensive Obsidian-compatible documentation for the <code>claude_prompt.py</code> file:</p> <p>tags: [documentation, auto-generated] created: 2023-05-09</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/root/claude_prompt/#training-scripts-documentation","title":"Training Scripts Documentation","text":"<p>This documentation was automatically generated from the codebase.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/root/claude_prompt/#modules","title":"Modules","text":"<ul> <li>root</li> <li>claude_prompt</li> </ul> <p>tags: [documentation, module, auto-generated] module:  created: 2023-05-09</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/root/claude_prompt/#root-documentation","title":"Root Documentation","text":"<p>This module contains the following files:</p> <ul> <li>root/claude_prompt</li> </ul> <p>tags: [documentation, auto-generated] created: 2023-05-09</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/root/claude_prompt/#claude_promptpy-documentation","title":"claude_prompt.py Documentation","text":""},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/root/claude_prompt/#overview","title":"Overview","text":"<p>This Python script is designed to analyze code files and generate comprehensive Obsidian-compatible documentation. It uses the Anthropic API to generate the documentation based on a predefined prompt.</p> <p>The script processes all Python files in the current directory and its subdirectories, excluding test files and certain directories. It then creates a hierarchical structure of Obsidian-formatted documentation, including a README.md index file and individual files for each module and code file.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/root/claude_prompt/#key-components","title":"Key Components","text":""},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/root/claude_prompt/#importing-dependencies","title":"Importing Dependencies","text":"<p>The script starts by importing the necessary modules, including <code>os</code>, <code>json</code>, <code>glob</code>, <code>sys</code>, and <code>anthropic</code>.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/root/claude_prompt/#configuring-the-anthropic-client","title":"Configuring the Anthropic Client","text":"<p>The script sets up the Anthropic client using the API key stored in the environment variable <code>ANTHROPIC_API_KEY</code>.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/root/claude_prompt/#selecting-files-to-process","title":"Selecting Files to Process","text":"<p>The script determines the files to process based on the command-line arguments or by finding all Python files in the current directory and its subdirectories, excluding certain directories and test files.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/root/claude_prompt/#generating-documentation","title":"Generating Documentation","text":"<p>The main loop of the script processes each file, creating a prompt for the Anthropic API and then extracting the generated documentation. The documentation is stored in a dictionary, with the keys based on the module structure.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/root/claude_prompt/#writing-documentation-to-files","title":"Writing Documentation to Files","text":"<p>The script creates the necessary directory structure and writes the documentation to Markdown files in the <code>generated_docs</code> directory. It first creates a README.md index file, then writes individual module and file documentation.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/root/claude_prompt/#dependencies-and-design-patterns","title":"Dependencies and Design Patterns","text":"<p>The script relies on the Anthropic API to generate the documentation, which is a key architectural choice. It also follows a modular design, with the main logic separated into functions and the file processing and documentation writing handled in separate sections.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/root/claude_prompt/#related-files","title":"Related Files","text":"<p>This script is likely part of a larger codebase, and there may be other files or modules that are related to the documentation generation process. Some potential related files could be:</p> <ul> <li>root/README.md</li> <li>root/utils.py</li> <li>root/config.py</li> </ul> <p>[!note] The documentation generation process is automated, so any changes to the codebase may require updating the documentation accordingly.</p>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/root/generate_training_data/","title":"Generate training data","text":"<pre><code>---\ntags: [Python, Data Generation, Machine Learning, Obsidian]\n---\n\n# generate_training_data.py\n\nThis Python script is responsible for generating training data for a machine learning model. It extracts code snippets from various repositories, creates training and validation data pairs, and writes them to separate files. The script also provides a progress dashboard and estimates the cost of the data generation process.\n\n## Functions\n\n### `get_file_size(filepath)`\nThis function takes a file path as input and returns the file size in megabytes (MB). If the file does not exist, it returns 0.\n\n### `main()`\nThis is the main function that orchestrates the entire data generation process. It performs the following steps:\n\n1. Estimates the total token usage and cost before running the process.\n2. Prompts the user to confirm the execution.\n3. Extracts the code documents from the specified repositories.\n4. Sets up the progress bars and the user interface.\n5. Iterates through the extracted documents, creates training pairs, and updates the statistics.\n6. Splits the generated pairs into training and validation sets.\n7. Writes the training and validation data to separate files.\n8. Displays the final dashboard and logs the completion information.\n\n## Key Components\n\n1. **Data Extraction**: The script uses the `extract_docs` function from the `data_generation.extractor` module to extract code documents from the specified repositories.\n2. **Training Data Generation**: The `create_training_pairs` function from the `data_generation.training_generator` module is used to generate the training and validation data pairs.\n3. **Progress Tracking**: The script utilizes the `setup_progress`, `get_dashboard`, `start_ascii_animation`, and `initialize_repo_tasks` functions from the `data_generation.progress_ui` module to provide a comprehensive progress dashboard.\n4. **File I/O**: The script writes the training and validation data to separate files specified by the `TRAINING_FILE` and `VALIDATION_FILE` constants.\n\n## Dependencies\n\nThe script relies on the following external libraries:\n\n- `os`: For file and directory operations.\n- `json`: For handling JSON data.\n- `random`: For shuffling the generated data pairs.\n- `logging`: For logging the completion information.\n- `rich`: For creating the progress dashboard and user interface.\n\n## Design Patterns\n\nThe script follows the **Separation of Concerns** design pattern by separating the data extraction, training data generation, and progress tracking into different modules and functions.\n\n## Related Files\n\n- [data_generation.extractor](data_generation.extractor.md)\n- [data_generation.training_generator](data_generation.training_generator.md)\n- [data_generation.progress_ui](data_generation.progress_ui.md)\n\n&gt; [!note]\n&gt; This script is part of the data generation process for a machine learning model. It should be used in conjunction with other components of the project.\n\n```python\n# generate_training_data.py\nimport os\nimport json\nimport random\nimport logging\nfrom rich.live import Live\nfrom rich.panel import Panel\nfrom data_generation.extractor import extract_docs, REPOS\nfrom data_generation.training_generator import create_training_pairs, estimate_task_cost\nfrom data_generation.progress_ui import (\n    setup_progress,\n    get_dashboard,\n    console,\n    start_ascii_animation,\n    initialize_repo_tasks\n)\n\n# Configuration\nTRAINING_SPLIT = 0.8\nTRAINING_FILE = \"../training-data/training_data.jsonl\"\nVALIDATION_FILE = \"../training-data/validation_data.jsonl\"\n\n# Function definitions\ndef get_file_size(filepath):\n    \"\"\"Get the file size in MB.\"\"\"\n    return os.path.getsize(filepath) / (1024 * 1024) if os.path.exists(filepath) else 0\n\ndef main():\n    \"\"\"Main function to process repositories and generate training data.\"\"\"\n    # Estimate token usage &amp; cost before running\n    # ... (code omitted for brevity)\n</code></pre>"},{"location":"EHS%20Dashboard%20%28Public%29/LLM-Training-Scripts/root/sample/","title":"Sample","text":"<pre><code>---\ntags: [Python, OpenAI, API]\n---\n\n# sample.py - OpenAI API Interaction\n\nThis Python script demonstrates how to interact with the OpenAI API to generate a simple text response. It loads the API key from the environment, initializes the OpenAI client, and makes a test API call to the GPT-4 language model.\n\n## Dependencies\n\nThis script relies on the following dependencies:\n\n- `openai` - Python client library for the OpenAI API\n- `os` - For accessing environment variables\n\n## Functions\n\n### `load_api_key()`\n\nLoads the OpenAI API key from the environment variable `OPENAI_API_KEY`. If the API key is not found, it prints an error message and exits the script.\n\n&gt; [!note]\n&gt; Make sure to set the `OPENAI_API_KEY` environment variable before running the script.\n\n### `test_api_call()`\n\nMakes a test API call to the OpenAI API using the `gpt-4-turbo` model. It sends a simple \"Say hello\" message and prints the response.\n\n## Usage\n\nTo use this script, follow these steps:\n\n1. Ensure you have the `openai` library installed (`pip install openai`).\n2. Set the `OPENAI_API_KEY` environment variable with your OpenAI API key.\n3. Run the script using `python sample.py`.\n\nThe script will output the response from the OpenAI API, or an error message if the API call fails.\n\n## Related Files\n\n- [openai_utils.py](openai_utils.py.md) - Utility functions for interacting with the OpenAI API\n- [language_model_comparison.md](language_model_comparison.md.md) - Comparison of different language models and their use cases\n\n```python\nimport openai\nimport os\n\n# Load API Key\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    print(\"\u274c ERROR: OpenAI API key is missing. Set OPENAI_API_KEY in your environment variables.\")\n    exit(1)\n\n# Initialize client\nclient = openai.OpenAI(api_key=api_key)\n\n# Test API call\ntry:\n    response = client.chat.completions.create(\n        model=\"gpt-4-turbo\",\n        messages=[{\"role\": \"system\", \"content\": \"Say hello.\"}],\n        temperature=0.7,\n        max_tokens=50\n    )\n    print(\"\u2705 OpenAI API Response:\", response.choices[0].message.content)\nexcept Exception as e:\n    print(\"\u274c ERROR calling OpenAI API:\", str(e))\n</code></pre>"},{"location":"Higher-Elevation-Software/Higher%20Elevation%20Software%20Wiki/","title":"Higher Elevation Software Wiki","text":"","tags":[]},{"location":"Higher-Elevation-Software/Higher%20Elevation%20Software%20Wiki/#documentation-coming-soon","title":"\ud83d\udea7 Documentation Coming Soon","text":"","tags":[]},{"location":"Higher-Elevation-Software/Higher%20Elevation%20Software%20Wiki/#publish-me","title":"publish-me","text":"<p>We're actively working to provide comprehensive documentation covering:</p> <ul> <li> <p>Integrations: Step-by-step guides and references for integrating our software with third-party platforms and services.</p> </li> <li> <p>API Documentation: Detailed API endpoints, authentication methods, usage examples, and best practices.</p> </li> <li> <p>Upcoming Features: A roadmap of future enhancements and features we're developing to improve our software suite.</p> </li> </ul> <p>Stay tuned as we prepare these resources. Your patience and interest are greatly appreciated.</p>","tags":[]},{"location":"Higher-Elevation-Software/Higher%20Elevation%20Software%20Wiki/#questions-or-suggestions","title":"\ud83d\udcec Questions or Suggestions","text":"<p>Feel free to reach out with questions or suggestions through our support channels.</p> <p>Thank you for your support!</p>","tags":[]},{"location":"docs/","title":"Higher Elevation Software Documentation","text":""},{"location":"docs/#welcome-to-the-official-documentation-site-for-higher-elevation-software","title":"Welcome to the official documentation site for Higher Elevation Software.","text":"<p>Documentation on integrations, API, and upcoming features is coming soon.</p> <p>Stay tuned!</p>"}]}